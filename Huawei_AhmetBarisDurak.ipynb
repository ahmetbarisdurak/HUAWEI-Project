{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ-Gw9lUuZ0E"
      },
      "source": [
        "<img src=\"assets/img/huawei_logo.png\"  style=\"width:160px;\"/>\n",
        "<div style=\"background-color: white; padding: 10px; border-bottom: 6px solid #C2172D;\">\n",
        "    <h2 style=\"color: black\" id=\"introduction\">Batch Data Processing with Apache Spark</h2>\n",
        "    <p>Tolgahan Cepel - Mert Akat</p>\n",
        "    <p></p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG2PHb_suZ0G"
      },
      "source": [
        "## [Contents](#contents)\n",
        "1. [Introduction](#introduction)\n",
        "2. [Importing the libraries](#library)\n",
        "3. [Reading the data](#read_data)\n",
        "4. [SparkSQL API Practices](#spark_sql_practices)\n",
        "   * [Selecting columns](#selecting_columns)\n",
        "   * [Data manipulation](#data_manipulation)\n",
        "   * [Filtering rows](#filtering_rows)\n",
        "   * [Aggregating data](#aggregating_data)\n",
        "   * [Joining](#joining)\n",
        "5. [Case Studies](#assignments)\n",
        "   * [Assignment 1: Jacket sales per region](#assignment_1)\n",
        "   * [Assignment 2: Maximum turnover of the retailer regions](#assignment_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS7wt9ZuuZ0H"
      },
      "source": [
        "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
        "    <a id=\"introduction\">\n",
        "        <h3 style=\"color: #C2172D\">1. Introduction</h3>\n",
        "    </a>  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2YBu3ZVuZ0H"
      },
      "source": [
        "<img src=\"assets/img/data_model.svg\"  style=\"width:1000px; padding: 20px\"/>\n",
        "\n",
        "#### SQL Tables Description\n",
        "- **FactSale:** Sales transactions fact table\n",
        "- **FactPurchase:** Purchases fact table\n",
        "- **DimRetailer:** Retailer details dimension table\n",
        "- **DimCustomer:** Customer details dimension table\n",
        "- **DimProduct:** Product details dimension table\n",
        "- **DimRegion:** Region details dimension table\n",
        "- **DimDate:** Date dimension table\n",
        "- **DimSupplier:** Supplier details dimension table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "WSKUHXfvuv8X",
        "outputId": "2833fc3a-df16-46fd-db34-fe4d165f8739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [830 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,069 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,374 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Ign:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,798 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n",
            "Fetched 7,480 kB in 52s (144 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=fdf76d8d88df3d0aa2773901da0c50609e93f3a240655fc76131eef2db6c9494\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://37ab7dec155b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x797c5811f7c0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nthnpitcuZ0H"
      },
      "source": [
        "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
        "    <a id=\"library\">\n",
        "        <h3 style=\"color: #C2172D\">2. Importing the libraries</h3>\n",
        "    </a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcfTjCTLuZ0H"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession, functions as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc8HdkoquZ0I"
      },
      "source": [
        "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
        "    <a id=\"read_data\">\n",
        "        <h3 style=\"color: #C2172D\">3. Reading the data</h3>\n",
        "    </a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDgDc8JnuZ0I"
      },
      "outputs": [],
      "source": [
        "# Creating new SparkSession instance\n",
        "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
        "\n",
        "# Reading parquet data and assigning to DataFrame variables\n",
        "df_pur = spark.read.parquet(\"/content/drive/MyDrive/DEM DATA/data/purchase\")\n",
        "df_sal = spark.read.parquet(\"/content/drive/MyDrive/DEM DATA/data/sale\")\n",
        "df_cus = spark.read.parquet(\"/content/drive/MyDrive/DEM DATA/data/customer\")\n",
        "df_ret = spark.read.parquet(\"/content/drive/MyDrive/DEM DATA/data/retailer\")\n",
        "df_pro = spark.read.parquet(\"/content/drive/MyDrive/DEM DATA/data/product\")\n",
        "df_sup = spark.read.parquet(\"/content/drive/MyDrive/DEM DATA/data/supplier\")\n",
        "df_reg = spark.read.parquet(\"/content/drive/MyDrive/DEM DATA/data/region\")\n",
        "df_date = spark.read.parquet(\"/content/drive/MyDrive/DEM DATA/data/date\")\n",
        "\n",
        "# Creating temporary view tables for Spark SQL queries\n",
        "df_cus.createOrReplaceTempView(\"DimCustomer\")\n",
        "df_pur.createOrReplaceTempView(\"FactPurchase\")\n",
        "df_sal.createOrReplaceTempView(\"FactSale\")\n",
        "df_ret.createOrReplaceTempView(\"DimRetailer\")\n",
        "df_pro.createOrReplaceTempView(\"DimProduct\")\n",
        "df_sup.createOrReplaceTempView(\"DimSupplier\")\n",
        "df_reg.createOrReplaceTempView(\"DimRegion\")\n",
        "df_date.createOrReplaceTempView(\"DimDate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6E50cxRuZ0I"
      },
      "source": [
        "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
        "    <a id=\"spark_sql_practices\">\n",
        "        <h3 style=\"color: #C2172D\">4. Spark SQL Practices</h3>\n",
        "    </a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIfKczGuuZ0I"
      },
      "source": [
        "**<a id=\"selecting_columns\">Selecting columns</a>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoVC-z6XuZ0I",
        "outputId": "69ff95f7-ccaa-45fe-e77a-740be11db056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+--------+----------+\n",
            "|customer_id|   name| surname|birth_date|\n",
            "+-----------+-------+--------+----------+\n",
            "|          1| Jazmin|  Burril|1958-09-22|\n",
            "|          2| Dalila|   Faers|2000-11-08|\n",
            "|          3|Wayland|Walework|1976-03-08|\n",
            "|          4|Amberly|  Haquin|1948-10-08|\n",
            "|          5|Garrett|   Frear|1957-09-25|\n",
            "+-----------+-------+--------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT customer_id, name, surname, birth_date FROM DimCustomer LIMIT 5\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UklTbpvVuZ0J",
        "outputId": "57b892ea-626a-47ba-b772-b514f77ed463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+--------+----------+\n",
            "|customer_id|   name| surname|birth_date|\n",
            "+-----------+-------+--------+----------+\n",
            "|          1| Jazmin|  Burril|1958-09-22|\n",
            "|          2| Dalila|   Faers|2000-11-08|\n",
            "|          3|Wayland|Walework|1976-03-08|\n",
            "|          4|Amberly|  Haquin|1948-10-08|\n",
            "|          5|Garrett|   Frear|1957-09-25|\n",
            "+-----------+-------+--------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_cus.select(\"customer_id\", \"name\", \"surname\", \"birth_date\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB7XnPeLuZ0J"
      },
      "source": [
        "**<a id=\"data_manipulation\">Data manipulation: </a>** Calculating the ages from date of birth data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYjr4eNEuZ0J",
        "outputId": "3e8e217e-5070-4391-eb9f-4eea5383b022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+--------+---+\n",
            "|customer_id|   name| surname|age|\n",
            "+-----------+-------+--------+---+\n",
            "|          1| Jazmin|  Burril| 66|\n",
            "|          2| Dalila|   Faers| 24|\n",
            "|          3|Wayland|Walework| 48|\n",
            "|          4|Amberly|  Haquin| 76|\n",
            "|          5|Garrett|   Frear| 67|\n",
            "+-----------+-------+--------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    customer_id\n",
        "    ,name\n",
        "    ,surname\n",
        "    ,YEAR(CURRENT_DATE()) - YEAR(birth_date) AS age\n",
        "FROM DimCustomer\n",
        "LIMIT 5\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgZLee6RuZ0K",
        "outputId": "6a6190ed-4c59-4b85-c53a-63cc5b72f7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+--------+---+\n",
            "|customer_id|   name| surname|age|\n",
            "+-----------+-------+--------+---+\n",
            "|          1| Jazmin|  Burril| 66|\n",
            "|          2| Dalila|   Faers| 24|\n",
            "|          3|Wayland|Walework| 48|\n",
            "|          4|Amberly|  Haquin| 76|\n",
            "|          5|Garrett|   Frear| 67|\n",
            "+-----------+-------+--------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "(\n",
        "    df_cus.withColumn(\"age\", F.year(F.current_date()) - F.year(\"birth_date\"))\n",
        "    .select(\"customer_id\", \"name\", \"surname\", \"age\")\n",
        "    .show(5)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdoYxYfhuZ0K"
      },
      "source": [
        "**<a id=\"filtering_rows\">Filtering rows</a>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x1dFoKTuZ0K",
        "outputId": "2048c81a-3cb2-4eb2-f58d-11dfc13acdac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------+---+\n",
            "|   name| surname|age|\n",
            "+-------+--------+---+\n",
            "| Jazmin|  Burril| 66|\n",
            "|Wayland|Walework| 48|\n",
            "|Amberly|  Haquin| 76|\n",
            "|Garrett|   Frear| 67|\n",
            "|  Horst|   Isted| 49|\n",
            "+-------+--------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    name\n",
        "    ,surname\n",
        "    ,age\n",
        "FROM\n",
        "(\n",
        "    SELECT\n",
        "        customer_id\n",
        "        ,name\n",
        "        ,surname\n",
        "        ,YEAR(CURRENT_DATE()) - YEAR(birth_date) AS age\n",
        "    FROM DimCustomer\n",
        ")\n",
        "WHERE age >= 30\n",
        "LIMIT 5\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ktE5-sfuZ0K",
        "outputId": "18c3521e-2628-4cb5-9f54-5249caf4c61a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------+---+\n",
            "|   name| surname|age|\n",
            "+-------+--------+---+\n",
            "| Jazmin|  Burril| 66|\n",
            "|Wayland|Walework| 48|\n",
            "|Amberly|  Haquin| 76|\n",
            "|Garrett|   Frear| 67|\n",
            "|  Horst|   Isted| 49|\n",
            "+-------+--------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "(\n",
        "    df_cus.withColumn(\"age\", F.year(F.current_date()) - F.year(\"birth_date\"))\n",
        "    .select(\"name\", \"surname\", \"age\")\n",
        "    .filter(F.col(\"age\") >= 30)\n",
        "    .show(5)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGWDJrebuZ0K"
      },
      "source": [
        "**<a id=\"aggregating_data\">Aggregating data</a>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcTF5oUOuZ0K",
        "outputId": "490eb2ce-8786-4396-a7a3-2ecefe7b1a0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------------+------------+\n",
            "|order_id|total_quantity|total_amount|\n",
            "+--------+--------------+------------+\n",
            "|    3647|            13|         521|\n",
            "|    2574|            13|         488|\n",
            "|    3515|            13|         402|\n",
            "|     101|            12|         359|\n",
            "|     440|            12|         426|\n",
            "|    3763|            12|         323|\n",
            "|    1585|            12|         488|\n",
            "|    3289|            12|         327|\n",
            "|    1382|            11|         452|\n",
            "|    1752|            11|         298|\n",
            "+--------+--------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    order_id\n",
        "    ,SUM(quantity) AS total_quantity\n",
        "    ,SUM(total_amt) AS total_amount\n",
        "FROM FactSale\n",
        "GROUP BY order_id\n",
        "ORDER BY total_quantity DESC\n",
        "LIMIT 10\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRRA8hPwuZ0L",
        "outputId": "fd7cd8f6-c4df-490d-f450-c22fedff70c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------------+------------+\n",
            "|order_id|total_quantity|total_amount|\n",
            "+--------+--------------+------------+\n",
            "|    3647|            13|         521|\n",
            "|    2574|            13|         488|\n",
            "|    3515|            13|         402|\n",
            "|     101|            12|         359|\n",
            "|     440|            12|         426|\n",
            "|    3763|            12|         323|\n",
            "|    1585|            12|         488|\n",
            "|    3289|            12|         327|\n",
            "|    2337|            11|         357|\n",
            "|    3743|            11|         359|\n",
            "+--------+--------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "(\n",
        "    df_sal.groupBy(\"order_id\").agg(\n",
        "        F.sum(\"quantity\").alias(\"total_quantity\"),\n",
        "        F.sum(\"total_amt\").alias(\"total_amount\")\n",
        "    ).orderBy(\"total_quantity\", ascending=False)\n",
        "    .show(10)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTVtZqexuZ0L"
      },
      "source": [
        "**<a id=\"joining\">Joining</a>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOAFqs-RuZ0L",
        "outputId": "66519591-b29d-4e83-a68a-039e1007d784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+------------------+\n",
            "|      region_name|               age|\n",
            "+-----------------+------------------+\n",
            "|          Akdeniz| 50.81521739130435|\n",
            "|     Dogu Anadolu| 50.13095238095238|\n",
            "|Guneydogu Anadolu| 48.58119658119658|\n",
            "|          Marmara|48.189542483660134|\n",
            "|       Ic Anadolu| 48.07772020725388|\n",
            "|        Karadeniz| 47.75121951219512|\n",
            "|              Ege|46.888888888888886|\n",
            "+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    region_name\n",
        "    ,AVG(YEAR(CURRENT_DATE()) - YEAR(birth_date)) AS age\n",
        "FROM DimCustomer cus\n",
        "INNER JOIN DimRegion reg\n",
        "ON cus.city_id = reg.city_id\n",
        "GROUP BY region_name\n",
        "ORDER BY age DESC\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0c6QInKuZ0L",
        "outputId": "7a2a7691-1cfe-4116-931a-20b5f146afae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+------------------+\n",
            "|      region_name|               age|\n",
            "+-----------------+------------------+\n",
            "|          Akdeniz| 50.81521739130435|\n",
            "|     Dogu Anadolu| 50.13095238095238|\n",
            "|Guneydogu Anadolu| 48.58119658119658|\n",
            "|          Marmara|48.189542483660134|\n",
            "|       Ic Anadolu| 48.07772020725388|\n",
            "|        Karadeniz| 47.75121951219512|\n",
            "|              Ege|46.888888888888886|\n",
            "+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "(\n",
        "    df_cus\n",
        "    .join(df_reg, df_cus.city_id == df_reg.city_id)\n",
        "    .groupBy(\"region_name\").agg(\n",
        "        F.avg(F.year(F.current_date()) - F.year(\"birth_date\")).alias(\"age\")\n",
        "    )\n",
        "    .orderBy(\"age\", ascending=False)\n",
        "    .show()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FGd2MpYuZ0L"
      },
      "source": [
        "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
        "    <a id=\"case_studies\">\n",
        "        <h3 style=\"color: #C2172D\">5. Case Studies</h3>\n",
        "    </a>  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NUEv4HauZ0L"
      },
      "source": [
        "<div style=\"background-color: white; padding: 10px;\">\n",
        "    <a id=\"assignment_1\">\n",
        "        <h4 style=\"color: #0D9276\">Assignment 1: Jacket sales per region</h3>\n",
        "    </a>\n",
        "</div>\n",
        "<br>\n",
        "<h4>\n",
        "    Write SparkSQL and Python API scripts that results: Region-based total quantity and amount of jacket sales between June and August 2023.\n",
        "</h4>\n",
        "<p>The expected out is as follows: </p>\n",
        "\n",
        "| region_name       | product_type | total_quantity | total_amount |   |\n",
        "|-------------------|--------------|----------------|--------------|---|\n",
        "| Marmara           | Jacket       | 213            | 8358         |   |\n",
        "| Dogu Anadolu      | Jacket       | 284            | 11547        |   |\n",
        "| Guneydogu Anadolu | Jacket       | 176            | 6981         |   |\n",
        "| Ic Anadolu        | Jacket       | 260            | 10496        |   |\n",
        "| Akdeniz           | Jacket       | 162            | 6637         |   |\n",
        "| Karadeniz         | Jacket       | 310            | 12582        |   |\n",
        "| Ege               | Jacket       | 101            | 3953      \n",
        "\n",
        "\n",
        "### External links\n",
        "- https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html\n",
        "- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html\n",
        "- https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html   |   |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_Urhhlk1eRB",
        "outputId": "206964c2-2025-4e0f-abde-130b4727b05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+------------+--------------+------------+\n",
            "|      region_name|product_type|total_quantity|total_amount|\n",
            "+-----------------+------------+--------------+------------+\n",
            "|          Marmara|      Jacket|           213|        8358|\n",
            "|     Dogu Anadolu|      Jacket|           284|       11547|\n",
            "|Guneydogu Anadolu|      Jacket|           176|        6981|\n",
            "|       Ic Anadolu|      Jacket|           260|       10496|\n",
            "|          Akdeniz|      Jacket|           162|        6637|\n",
            "|        Karadeniz|      Jacket|           310|       12582|\n",
            "|              Ege|      Jacket|           101|        3953|\n",
            "+-----------------+------------+--------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"\n",
        "SELECT\n",
        "    reg.region_name,\n",
        "    p.product_type,\n",
        "    SUM(s.quantity) AS total_quantity,\n",
        "    SUM(s.total_amt) AS total_amount\n",
        "FROM\n",
        "    FactSale s\n",
        "INNER JOIN DimProduct p ON s.product_id = p.product_id\n",
        "INNER JOIN DimDate d ON s.date = d.date\n",
        "JOIN DimCustomer cus ON s.customer_id = cus.customer_id\n",
        "JOIN DimRegion reg ON cus.city_id = reg.city_id\n",
        "WHERE\n",
        "    p.product_type = 'Jacket'\n",
        "    AND d.date BETWEEN '2023-06-01' AND '2023-08-31'\n",
        "GROUP BY\n",
        "    reg.region_name, p.product_type\n",
        "\"\"\"\n",
        "result_df = spark.sql(query)\n",
        "result_df.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxAwsUaNuZ0M",
        "outputId": "8aef5dd9-0395-422b-b0f8-bb37f3d9dbc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+------------+--------------+------------+\n",
            "|      region_name|product_type|total_quantity|total_amount|\n",
            "+-----------------+------------+--------------+------------+\n",
            "|          Marmara|      Jacket|           213|        8358|\n",
            "|     Dogu Anadolu|      Jacket|           284|       11547|\n",
            "|Guneydogu Anadolu|      Jacket|           176|        6981|\n",
            "|       Ic Anadolu|      Jacket|           260|       10496|\n",
            "|          Akdeniz|      Jacket|           162|        6637|\n",
            "|        Karadeniz|      Jacket|           310|       12582|\n",
            "|              Ege|      Jacket|           101|        3953|\n",
            "+-----------------+------------+--------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# 'Jacket' tipindeki ürünler için filtreleme ve tarih aralığına göre filtreleme yapıyoruz\n",
        "filtered_sales = df_sal \\\n",
        "    .join(df_pro, df_sal.product_id == df_pro.product_id) \\\n",
        "    .join(df_date, df_sal.date == df_date.date) \\\n",
        "    .join(df_cus, df_sal.customer_id == df_cus.customer_id) \\\n",
        "    .join(df_reg, df_cus.city_id == df_reg.city_id) \\\n",
        "    .filter((df_pro.product_type == \"Jacket\") & (df_date.date >= F.lit(\"2023-06-01\")) & (df_date.date <= F.lit(\"2023-08-31\")))\n",
        "\n",
        "# Bölge ve ürün tipine göre gruplama yaparak toplam miktarı ve toplam tutarı hesaplıyoruz\n",
        "result_df = filtered_sales.groupBy(\"region_name\", \"product_type\") \\\n",
        "    .agg(\n",
        "        F.sum(\"quantity\").alias(\"total_quantity\"),\n",
        "        F.sum(\"total_amt\").alias(\"total_amount\")\n",
        "    )\n",
        "\n",
        "# Sonuçları gösteriyor\n",
        "result_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi7osyCEuZ0M"
      },
      "source": [
        "<div style=\"background-color: white; padding: 10px;\">\n",
        "    <a id=\"assignment_2\">\n",
        "        <h4 style=\"color: #0D9276\">Assignment 2: Maximum turnover of the retailer regions</h3>\n",
        "    </a>\n",
        "</div>\n",
        "<br>\n",
        "<h4>\n",
        "    Find the maximum turnover region of each retailer, and obtain total amount for each retailer and region.\n",
        "</h4>\n",
        "<p>The expected out is as follows: </p>\n",
        "\n",
        "| retailer_id | retailer_name | region_name | total_amount |\n",
        "|-------------|---------------|-------------|--------------|\n",
        "| 1           | Hepsiburada   | Karadeniz   | 42642        |\n",
        "| 2           | Trendyol      | Ic Anadolu  | 71689        |\n",
        "| 3           | n11           | Ic Anadolu  | 11995        |\n",
        "| 4           | Gittigidiyor  | Karadeniz   | 16081        |\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp4QGgmKyXFc",
        "outputId": "683e4bc1-d2fc-4cde-c159-76d67ecaf398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------+-----------+------------+\n",
            "|retailer_id|retailer_name|region_name|total_amount|\n",
            "+-----------+-------------+-----------+------------+\n",
            "|          1|  Hepsiburada|  Karadeniz|       42642|\n",
            "|          2|     Trendyol| Ic Anadolu|       71689|\n",
            "|          3|          n11| Ic Anadolu|       11995|\n",
            "|          4| Gittigidiyor|  Karadeniz|       16081|\n",
            "+-----------+-------------+-----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"\n",
        "WITH RegionSales AS (\n",
        "    SELECT\n",
        "        ret.retailer_id,\n",
        "        ret.retailer_name,\n",
        "        reg.region_name,\n",
        "        SUM(s.total_amt) AS total_sales_amount\n",
        "    FROM\n",
        "        FactSale s\n",
        "    JOIN DimRetailer ret ON s.retailer_id = ret.retailer_id\n",
        "    JOIN DimCustomer cus ON s.customer_id = cus.customer_id\n",
        "    JOIN DimRegion reg ON cus.city_id = reg.city_id\n",
        "    GROUP BY\n",
        "        ret.retailer_id, ret.retailer_name, reg.region_name\n",
        "),\n",
        "MaxSales AS (\n",
        "    SELECT\n",
        "        retailer_id,\n",
        "        MAX(total_sales_amount) AS max_sales\n",
        "    FROM\n",
        "        RegionSales\n",
        "    GROUP BY\n",
        "        retailer_id\n",
        ")\n",
        "SELECT\n",
        "    m.retailer_id,\n",
        "    rs.retailer_name,\n",
        "    rs.region_name,\n",
        "    m.max_sales AS total_amount\n",
        "FROM\n",
        "    MaxSales m\n",
        "JOIN\n",
        "    RegionSales rs ON m.retailer_id = rs.retailer_id AND m.max_sales = rs.total_sales_amount\n",
        "ORDER BY\n",
        "    m.retailer_id\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "result_df = spark.sql(query)\n",
        "result_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajSkfNrguZ0M",
        "outputId": "521cadec-24b9-4c87-d085-43feae9af225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------+-----------+---------+\n",
            "|retailer_id|retailer_name|region_name|max_sales|\n",
            "+-----------+-------------+-----------+---------+\n",
            "|          1|  Hepsiburada|  Karadeniz|    42642|\n",
            "|          2|     Trendyol| Ic Anadolu|    71689|\n",
            "|          3|          n11| Ic Anadolu|    11995|\n",
            "|          4| Gittigidiyor|  Karadeniz|    16081|\n",
            "+-----------+-------------+-----------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Perakendeci ve bölgeye göre toplam satış miktarını hesaplamak için birleştirme ve agregasyon işlemi yapılıyor\n",
        "region_sales = df_sal.join(df_ret, \"retailer_id\") \\\n",
        "    .join(df_cus, \"customer_id\") \\\n",
        "    .join(df_reg, df_cus[\"city_id\"] == df_reg[\"city_id\"]) \\\n",
        "    .groupBy(\"retailer_id\", \"retailer_name\", \"region_name\") \\\n",
        "    .agg(F.sum(\"total_amt\").alias(\"total_sales_amount\"))\n",
        "\n",
        "# Her perakendeci için maksimum satış miktarını bulma\n",
        "max_sales = region_sales.groupBy(\"retailer_id\") \\\n",
        "    .agg(F.max(\"total_sales_amount\").alias(\"max_sales\"))\n",
        "\n",
        "# Maksimum satış miktarına ve ilgili bölgelere göre perakendecileri birleştirme\n",
        "result = max_sales.join(region_sales, (max_sales[\"retailer_id\"] == region_sales[\"retailer_id\"]) & \\\n",
        "                        (max_sales[\"max_sales\"] == region_sales[\"total_sales_amount\"])) \\\n",
        "    .select(max_sales[\"retailer_id\"], \"retailer_name\", \"region_name\", \"max_sales\") \\\n",
        "    .orderBy(\"retailer_id\")\n",
        "\n",
        "# Sonucu göster\n",
        "result.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jzfr2gJgxyCB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
